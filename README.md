# Neural-Network-Library



# activation functions:

- ReLu
- Sigmoid
- Linear

# loss functions:

- Binary Crossentropy
- Mse(Mean squared error)

# Layers:

- Input layer
- Dense layer

# Weight Initializations:

- Random Initialization
- He Initialization 

# Data Preprocessing 

- Feature scaling
- Adding Polynomial features
- undersampling(for class imbalance in dataset)
